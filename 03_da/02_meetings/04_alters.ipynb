{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Sharing All Data Received from Alters**\n",
    "\n",
    "Considering $U_a = \\frac{a+e}{D}$ and $U_e = \\frac{a}{a+e}$ result in Pareto-optimal deal:\n",
    "\n",
    "$U_e \\cdot U_a = \\frac{a}{D}$\n",
    "\n",
    "This yields that the optimal deal is only determined by the size of the shared alter data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "\n",
    "from multiprocessing.pool import Pool\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_ROOT = \"../01_simulation/04_results/\"\n",
    "SEEDS = [42, 1234, 1867, 613, 1001]\n",
    "TIME_LIMIT = 300\n",
    "FALL_BACK_TIME = 15 #seconds -> corresponds to max. 208.3 m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "if not(os.path.exists(\"08_shared_veh_data/alters\")):\n",
    "    os.makedirs(\"08_shared_veh_data/alters\")\n",
    "for s in SEEDS:\n",
    "    if not(os.path.exists(f\"08_shared_veh_data/alters/{s}\")):\n",
    "        os.makedirs(f\"08_shared_veh_data/alters/{s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading *test* vehicles:\n",
    "\n",
    "with open(\"veh_list.json\") as f:\n",
    "    veh_list  = json.load(f)\n",
    "\n",
    "test_vehicles = veh_list[\"test_vehs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_data = pd.read_csv(\"08_shared_veh_data/combined_dataset.csv\")\n",
    "p_data[\"time\"] = p_data[\"time\"].astype(int)\n",
    "p_data = p_data[p_data[\"veh_id\"].isin(test_vehicles)]\n",
    "receive_time = [-1]*len(p_data)\n",
    "p_data[\"receive_time\"] = receive_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_commuters(veh_id):\n",
    "    if veh_id.startswith(\"carIn\"):\n",
    "        return veh_id.split(\":\")[0]\n",
    "    return veh_id\n",
    "\n",
    "#reading moving simulation data:\n",
    "\n",
    "m_data = pd.DataFrame()\n",
    "for s in SEEDS:\n",
    "    filename = f\"{RESULTS_ROOT}/vehicle_positions_{s}.csv\"\n",
    "    mf = pd.read_csv(filename)\n",
    "    mf[\"seed\"] = [s]*len(mf)\n",
    "    m_data = pd.concat([m_data, mf])\n",
    "\n",
    "m_data[\"veh_id\"] = m_data[\"veh_id\"].apply(combine_commuters)\n",
    "m_data = m_data[m_data[\"veh_id\"].isin(test_vehicles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"edge_maps.json\") as f:\n",
    "    edge_maps = json.load(f)\n",
    "\n",
    "edge_to_idx = edge_maps[\"edge_to_idx\"]\n",
    "idx_to_edge = edge_maps[\"idx_to_edge\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_meeting_vehicles(m_data, meeting_time, seed, meeting_times, meeting_time_gap=TIME_LIMIT):\n",
    "    #collecting recently met vehicles:\n",
    "    \n",
    "    meetings = m_data[m_data[\"time\"] == meeting_time]\n",
    "    meetings = meetings[meetings[\"seed\"] == seed]\n",
    "\n",
    "    mets = []\n",
    "\n",
    "    #same edges:\n",
    "    for edge in edge_to_idx:\n",
    "        vehs = meetings[meetings[\"edge\"] == edge][\"veh_id\"].unique()\n",
    "        for i in range(len(vehs)):\n",
    "            for j in range(i+1, len(vehs)):\n",
    "                sender = vehs[i]\n",
    "                receiver = vehs[j]\n",
    "                #if not met yet or met long time ago:\n",
    "                if ((not((sender, receiver) in meeting_times)) or\n",
    "                    meeting_time - meeting_times[(sender, receiver)] > meeting_time_gap):\n",
    "                    mets.append((sender, receiver))\n",
    "                    mets.append((receiver, sender)) #they meet vice-versa\n",
    "\n",
    "    #opposed edges:\n",
    "    for edge in edge_to_idx:\n",
    "        #only \"reversed\" edges are processed, to avoid duplicated meetings:\n",
    "        if edge.startswith(\"-\"):\n",
    "            veh_edge = meetings[meetings[\"edge\"] == edge][\"veh_id\"].unique()\n",
    "            contra_edge = edge.split(\"-\")[1]\n",
    "            veh_contra = meetings[meetings[\"edge\"] == contra_edge][\"veh_id\"].unique()\n",
    "            for sender in veh_edge:\n",
    "                for receiver in veh_contra:\n",
    "                    #if not met yet or met long time ago:\n",
    "                    if ((not((sender, receiver) in meeting_times)) or\n",
    "                        meeting_time - meeting_times[(sender, receiver)] > meeting_time_gap):\n",
    "                        mets.append((sender, receiver))\n",
    "                        mets.append((receiver, sender)) #they meet vice-versa\n",
    "\n",
    "\n",
    "    return mets #vehicles at the same time, at the same place, not the 'ego' vehicle and not met recently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data_upon_meeting(senders_data, meeting_time, seed, time_limit=TIME_LIMIT, ego_time=FALL_BACK_TIME):\n",
    "    send_data = senders_data[senders_data[\"seed\"] == seed]\n",
    "    send_data = send_data[send_data[\"time\"] <= meeting_time]\n",
    "    send_data = send_data[send_data[\"time\"] >= meeting_time-time_limit]\n",
    "\n",
    "    alters_data = send_data[send_data[\"receive_time\"]>-1]\n",
    "    has_alters = True if len(alters_data)>0 else False\n",
    "    alters_data = pd.concat([alters_data, send_data[send_data[\"time\"] >= meeting_time-ego_time]],\n",
    "                            ignore_index=True)\n",
    "\n",
    "    return alters_data.drop(columns=[\"receive_time\"]), has_alters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def receive_data(args):\n",
    "    sender, receiver, time, seed, vehicles_kb = args\n",
    "\n",
    "    received_data, has_alters = collect_data_upon_meeting(vehicles_kb[sender], time, seed)\n",
    "\n",
    "    #storing shared data:\n",
    "    store_shared_data = {\n",
    "        \"sender\": sender,\n",
    "        \"receiver\": receiver,\n",
    "        \"time\": time,\n",
    "        \"data\": received_data.to_json(),\n",
    "        \"contains_alter\": has_alters\n",
    "    }\n",
    "\n",
    "    #fusing data into the receiver vehicle's dataset:\n",
    "    rec_t = [time]*len(received_data)\n",
    "    received_data[\"receive_time\"] = rec_t\n",
    "\n",
    "    updated_kb = pd.concat([vehicles_kb[receiver], received_data], ignore_index=True)\n",
    "    updated_kb = updated_kb.drop_duplicates(subset=\"hash\", ignore_index=True)\n",
    "\n",
    "    return store_shared_data, updated_kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_seed_script(seed):\n",
    "    ps_data = p_data[p_data[\"seed\"] == seed]\n",
    "    meeting_times = {}\n",
    "    vehicles_kb = {}\n",
    "    store_sharing = []\n",
    "\n",
    "    for veh in test_vehicles:\n",
    "        vehicles_kb[veh] = copy.deepcopy(ps_data[ps_data[\"veh_id\"] == veh])\n",
    "\n",
    "    for t in trange(min(ps_data[\"time\"]), max(ps_data[\"time\"])):\n",
    "        meetings = collect_meeting_vehicles(m_data, t, seed, meeting_times)\n",
    "        for sender, receiver in meetings:\n",
    "            arguments = [sender, receiver, t, seed, vehicles_kb]\n",
    "            meeting_times[(sender, receiver)] = t\n",
    "            \n",
    "            new_store, updated_kb = receive_data(arguments)\n",
    "            vehicles_kb[receiver] = copy.deepcopy(updated_kb)\n",
    "            store_sharing.append(new_store)\n",
    "\n",
    "    for veh in vehicles_kb:\n",
    "        vehicles_kb[veh].to_csv(f\"08_shared_veh_data/alters/{seed}/{veh}.csv\", index=False)\n",
    "    store_dict = {\n",
    "        \"shared_data\": store_sharing\n",
    "    }\n",
    "    with open(f\"08_shared_veh_data/alters/{seed}/shared_data.json\", \"w\") as f:\n",
    "        json.dump(store_dict, f)\n",
    "\n",
    "with Pool(5) as ps:\n",
    "    ps.map(per_seed_script, SEEDS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
